# BI Engineer Challenger - Sales ETL Project Solution

## 1. Data Architecture

![data_architecture](/images/data_architecture.png)


## 2. Dataset Description

The dataset contains information about an online shopping website that sells various products and the interactions that have occurred between the users and the products. The dataset can be used to analyze how the users interact with the products and how the products perform on the website. The dataset can also be used to calculate the revenue generated by the products, the conversion rate of the users from viewing to buying the products, the transition rate of the users from one event to another and the cart abandonment rate of the users who do not complete their orders. The dataset can help to identify the factors that influence the user behavior and the product performance and to optimize the website design and the discount strategy.

## 3. Data Gathering

The idea was to create an integration using Apache Airflow to unzip and upload locally stored data to an S3 Bucket within my AWS account. Once the files are loaded into the bucket, the data will be inserted into Snowflake's "RAW" schema through Snowpipe.

### 3.1 - AWS S3 Bucket

First, an S3 Bucket was created in my AWS account to store the **.csv** files. Also, the necessary roles and policies were created in IAM to integrate the S3 bucket with Snowflake. 

To see the steps on how to create and configure the S3 bucket [click here](configure_S3_and_permissions.md). 

If you donâ€™t have an AWS account yet [click here](https://github.com/data-talks-sydney/create-AWS-free-account).


### 3.2 - Snowflake Integration

The integration between Snowflake and AWS S3 Bucket was created through Snowpipe.

To see the steps on how to create a pipe in Snowflake [click here](Create_snowpipe.md). 

To see the SQL scrypt that I used to create the enviroment and the integration [click here](data_gathering_enviroment.sql).


### 3.3 - Apache Airflow

A DAG (Directed Acyclic Graph) was created to unzip the .csv files and upload them to an S3 Bucket on AWS, placing each file in its respective folder based on its name. Once the files are successfully loaded into the bucket, the .zip file will be moved to an "extracted" folder, and the .csv files will be moved to an "uploaded" folder.

#### Plugins/Packages used:
- apache-airflow
- apache-airflow-providers-amazon

To see the step by step to run Airflow via Docker-Desktop [click here](https://github.com/edonizeti/apache_airflow_using_docker_desktop/blob/main/README.md)

## 4. Data Cleaning

## 5. Data Transformation

## 6. Data Analysis

## 7. Data Visualisation

## 8. Tech Stack