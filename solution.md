# BI Engineer Challenger - Sales ETL Project Solution

## 1. Data Architecture

![data_architecture](/images/data_architecture.png)


## 2. Dataset Description

The dataset contains information about an online shopping website that sells various products and the interactions that have occurred between the users and the products. The dataset can be used to analyze how the users interact with the products and how the products perform on the website. The dataset can also be used to calculate the revenue generated by the products, the conversion rate of the users from viewing to buying the products, the transition rate of the users from one event to another and the cart abandonment rate of the users who do not complete their orders. The dataset can help to identify the factors that influence the user behavior and the product performance and to optimize the website design and the discount strategy.

## 3. Data Gathering

The idea was to create an integration using Apache Airflow to unzip and upload locally stored data to an S3 Bucket within my AWS account. Once the files are loaded into the bucket, the data will be inserted into Snowflake's "RAW" schema through Snowpipe.

### 3.1 - AWS S3 Bucket

First, an S3 Bucket was created in my AWS account to store the **.csv** files. Also, the necessary roles and policies were created in IAM to integrate the S3 bucket with Snowflake. 

To see the steps on how to create and configure the S3 bucket [click here](https://github.com/edonizeti/integration_S3_bucket_and_snowpipe). 

If you donâ€™t have an AWS account yet [click here](https://github.com/data-talks-sydney/create-AWS-free-account).


### 3.2 - Snowflake Integration, Database and Schemas

The integration between Snowflake and AWS S3 Bucket was created through Snowpipe.

3 different schemas were created within the database:

- RAW - to receive the raw data from S3.
- Staging - will receive clean RAW schema data.
- Analytics - Is the analytics-ready schema, optimized for efficient querying and visualization.

To see the steps on how to create a pipe in Snowflake [click here](https://github.com/edonizeti/integration_S3_bucket_and_snowpipe). 

### 3.3 - Apache Airflow

A DAG (Directed Acyclic Graph) was created to unzip the .csv files and upload them to an S3 Bucket on AWS, placing each file in its respective folder based on its name. Once the files are successfully loaded into the bucket, the .zip file will be moved to an "extracted" folder, and the .csv files will be moved to an "uploaded" folder.

#### Plugins/Packages used:
- apache-airflow
- apache-airflow-providers-amazon

To see the step by step to run Airflow via Docker-Desktop [click here](https://github.com/edonizeti/apache_airflow_using_docker_desktop/blob/main/README.md)

## 4. Data Cleaning/Transformation

In this stage, the idea is to use dbt to perform all data cleaning and processing tasks, so that they can be in a more organized form for analysis and dashboard creation.

The concept here involves extracting the raw, unprocessed data from the RAW schema, performing all the necessary data cleaning and transformations, and storing them in the STAGING schema. Subsequently, these processed data will be presented in a more organized manner in the ANALYTICS schema, enabling efficient querying and visualization through aggregation, filtering, and other manipulations.

## 5. Data Analysis and Data Visualisation

## 6. Tech Stack
